{
    "title": "Providedproperattributionisprovided,Googleherebygrantspermissionto",
    "summary": "In the Transformer this is reducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as describedinsection3.2. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. [38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,MaximKrikun,YuanCao,QinGao,KlausMacherey,etal. 12 Input-Input Layer5 AttentionVisualizations tI tI si si ni ni siht siht tirips tirips taht taht a a ytirojam ytirojam fo fo naciremA naciremA stnemnrevog stnemnrevog evah evah dessap dessap wen wen swal swal ecnis ecnis 9002 9002 gnikam gnikam eht eht noitartsiger noitartsiger ro ro gnitov gnitov ssecorp ssecorp erom erom tluciffid tluciffid .",
    "headings": [],
    "ranked_section_relevance": []
}